{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# DataQualityManager\r\n",
        "\r\n",
        "Il DataQualityManager:\r\n",
        "- carica ed interpreta le configurazioni a partire dal file YAML\r\n",
        "- coordina il processo di applicazione delle aspettative di qualità dei dati ai DataFrame Spark\r\n",
        "- interagisce con ExecutionStrategy, delegando l'esecuzione della expectations ad una strategia scelta, come sequenziale, parallela o basata su micro-batch\r\n",
        "- aggrega i risultati, raccoglie ed elabora i risultati delle strategie di esecuzione, incluse la gestione delle politiche di blocco e l'esecuzione delle procedure di fallback come definito nella configrazione"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "import yaml\n",
        "from pyspark.sql import DataFrame\n",
        "from typing import Dict, Any\n",
        "#from manager.validators.spark_dataframe_validator import SparkDataFrameValidator\n",
        "#from manager.strategies.execution_strategy import ExecutionStrategy\n",
        "import re\n",
        "import pprint\n",
        "from typing import Tuple\n",
        "\n",
        "# from azure.keyvault.secrets import SecretClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.core.exceptions import ResourceNotFoundError\n",
        "\n",
        "class DataQualityManager:\n",
        "    \"\"\"\n",
        "    Manages the application of data quality rules and expectations using Apache Spark and the great_expectations library.\n",
        "    \n",
        "    This class is responsible for managing and applying data quality expectations defined in a YAML configuration file.\n",
        "    It integrates with Apache Spark to apply these expectations on Spark DataFrames.\n",
        "\n",
        "    Attributes:\n",
        "        config_path (str): Path to the YAML configuration file.\n",
        "        config_data (Dict[str, Any]): Data loaded from the YAML configuration file.\n",
        "    \"\"\"\n",
        "\n",
        "    # Constants for YAML file tags \n",
        "    TAG_EXPECTATIONS: str = 'expectations'\n",
        "    TAG_EXPECTATIONS_CATALOG: str = 'expectations_catalog'\n",
        "    TAG_DATA_QUALITY_DIMENSIONS: str = 'data_quality_dimensions'\n",
        "    TAG_BLOCKING_EXPECTATIONS_POLICY: str = 'blocking_expectations_policy'\n",
        "    TAG_REFERENCE: str = 'reference'\n",
        "    TAG_NAME: str = 'name'\n",
        "    TAG_ERROR_HANDLING: str = 'error_handling' \n",
        "    TAG_FALLBACK_PROCEDURE: str = 'fallback_procedure'\n",
        "    TAG_BLOCKING_EXPECTATIONS: str = 'blocking_expectations'\n",
        "    TAG_HALT_IF_FAILED = 'execute_all_expectations_then_halt_if_failed'\n",
        "    STRATEGY_TYPE: str = 'basic'  # Può essere modificato in 'blocking_first', 'ml_based'\n",
        "\n",
        "    def __init__(self, config_path: str,sql_config_df, *args, **kwargs) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the DataQualityManager with the specified configuration file.\n",
        "        \n",
        "        Args:\n",
        "            config_path (str): Path to the YAML configuration file.\n",
        "        \"\"\"\n",
        "        self.args=args\n",
        "        self.kwargs=kwargs\n",
        "\n",
        "        self.external_references = {\n",
        "            \"keyargs\": {\n",
        "                \"type\": \"keyargs\",\n",
        "                \"logic\": self.get_secret_from_key_args\n",
        "            }\n",
        "        }\n",
        "        self.config_path = config_path\n",
        "        self.sql_config_df=  sql_config_df\n",
        "        self.config_data: Dict[str, Any] = self._load_and_resolve_yaml_config()\n",
        "        self.data_sources = self._read_data_sources()  # Legge e memorizza i data source\n",
        "        self.expectations_catalog: Dict[str, Any] = self._load_expectations_catalog()\n",
        "        self.input_path_dynamic = input_path_dynamic\n",
        "    def get_config_data(self):\n",
        "        return self.config_data\n",
        "    def _read_data_sources(self):\n",
        "        \"\"\"\n",
        "        Legge tutti i data source definiti nel tag \"data_sources\" del file YAML.\n",
        "        \"\"\"\n",
        "        data_sources = {}\n",
        "        for data_source_name, data_source_config in self.config_data.get('data_sources', {}).items():\n",
        "            data_source = DataSourceRegistry.get_data_source(data_source_config[\"type\"], data_source_config)\n",
        "            #print(f\"data:sources.items(): {self.config_data.get('data_sources', {}).items():} , data_source_name:{data_source_name}, data_source_config:{data_source_config}, data_source:{data_source}\")\n",
        "            if data_source:\n",
        "                data_sources[data_source_name] = data_source\n",
        "            else:\n",
        "                raise ValueError(f\"Data source '{data_source_name}' not registered or not found.\")\n",
        "        return data_sources    \n",
        "    \n",
        "\n",
        "    def get_secret_from_key_args(self,key:str) -> str:\n",
        "        \"\"\"\n",
        "        Retrieves a parameter from args.\n",
        "\n",
        "        :param key: The name of the args key.\n",
        "        :return: The value of the key.\n",
        "        \"\"\"\n",
        "        return self.kwargs.get(key,None)\n",
        "    \n",
        "\n",
        "    def _load_yaml_config(self):\n",
        "        \"\"\"\n",
        "        Legge il contenuto di un file YAML da Azure Data Lake Storage Gen2 tramite il percorso ABFS.\n",
        "\n",
        "        Parameters:\n",
        "        - file_path_abfs (str): Il percorso ABFS del file YAML su Azure Data Lake Storage Gen2.\n",
        "\n",
        "        Returns:\n",
        "        - yaml_data: Il dizionario contenente i dati del file YAML.\n",
        "        \"\"\"\n",
        "\n",
        "        # Leggi il contenuto del file YAML come DataFrame\n",
        "        yaml_content = spark.read.text(self.config_path)\n",
        "\n",
        "        # Estrai il testo grezzo dalla colonna 'value'\n",
        "        raw_yaml_text = '\\n'.join(yaml_content.select(\"value\").rdd.flatMap(lambda x: x).collect())\n",
        "\n",
        "        # Rimuovi eventuali virgole che potrebbero causare errori nella conversione YAML\n",
        "        #raw_yaml_text = raw_yaml_text.replace(',', '')\n",
        "\n",
        "        # Carica il contenuto YAML in una struttura dati Python\n",
        "        yaml_data = yaml.safe_load(raw_yaml_text)\n",
        "\n",
        "        return yaml_data\n",
        "    \n",
        "    @staticmethod\n",
        "    def _extract_data_source_info(path: str) -> Tuple[str, str]:\n",
        "        match = re.match(r\"\\#\\{data_sources:([^}]+)\\}(.+)\", path)\n",
        "        if match:\n",
        "            return match.groups()\n",
        "        raise ValueError(f\"Invalid data source path: {path}\")\n",
        "\n",
        "\n",
        "    \n",
        "    import logging\n",
        "\n",
        "    \n",
        "    def _load_expectations_catalog(self):\n",
        "        \"\"\"\n",
        "        Carica il catalogo delle aspettative utilizzando i data source letti precedentemente.\n",
        "        Restituisce un dizionario in cui ogni chiave corrisponde a un'aspettativa e ogni valore\n",
        "        è il contenuto letto dalla sorgente dati associata.\n",
        "\n",
        "        Solleva:\n",
        "            KeyError: Se TAG_EXPECTATIONS_CATALOG non è presente in config_data.\n",
        "            ValueError: Se ci sono problemi con l'estrazione delle informazioni del data source.\n",
        "\n",
        "        Returns:\n",
        "            dict: Il catalogo delle aspettative.\n",
        "        \"\"\"\n",
        "        logger = logging.getLogger(__name__)\n",
        "        catalog = {}\n",
        "        # Verifica che TAG_EXPECTATIONS_CATALOG sia presente in config_data\n",
        "        if self.TAG_EXPECTATIONS_CATALOG not in self.config_data:\n",
        "            logger.error(f'{self.TAG_EXPECTATIONS_CATALOG} not found in config_data')\n",
        "            #raise KeyError(f'{self.TAG_EXPECTATIONS_CATALOG} not found in config_data')\n",
        "            return catalog\n",
        "\n",
        "        else:       \n",
        "            for key, path in self.config_data.get(self.TAG_EXPECTATIONS_CATALOG, {}).items():\n",
        "                try:\n",
        "                    # Utilizza il metodo extract_data_source_info in DataSourceRegistry\n",
        "                    data_source_type, path_suffix = DataSourceRegistry.extract_data_source_info(path)\n",
        "                    data_source = self.data_sources.get(data_source_type)\n",
        "\n",
        "                    if not data_source:\n",
        "                        raise ValueError(f\"Data source '{data_source_type}' not found in loaded data sources.\")\n",
        "\n",
        "                    # Utilizza il data source per leggere i dati relativi al catalogo delle aspettative\n",
        "                    catalog_content = data_source.read_data(path_suffix)\n",
        "                    if catalog_content is None:\n",
        "                        raise ValueError(\"Catalog content is None\")\n",
        "\n",
        "                    catalog[key] = catalog_content\n",
        "\n",
        "                except ValueError as ve:\n",
        "                    logger.error(f\"Error processing catalog entry '{key}': {ve}\")\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Unexpected error processing catalog entry '{key}': {e}\")\n",
        "                    continue\n",
        "\n",
        "            return catalog\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "    \n",
        "\n",
        "    def _resolve_external_references(self, yaml_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Risolve i riferimenti esterni nel file YAML.\n",
        "\n",
        "        Sample:\n",
        "        \n",
        "        data_sources:\n",
        "            my_secure_blob:\n",
        "                type: \"azure-blob\"\n",
        "                container_name: \"mycontainer\"\n",
        "                account_name: \"${keyvault:azure_storage_account_name}\"\n",
        "                sas_token: \"${keyvault:azure_storage_sas_token}\"\n",
        "                file_format: \"parquet\"\n",
        "                options:\n",
        "                option1: \"value1\"\n",
        "                option2: \"value2\"\n",
        "\n",
        "        :param yaml_data: Dizionario contenente i dati YAML.\n",
        "        :return: Dizionario con i riferimenti esterni risolti.\n",
        "        \"\"\"\n",
        "\n",
        "        regex = r\"\\$\\{(?P<key>[^}]+)\\}\"\n",
        "\n",
        "        def resolve_value(value: str) -> str:\n",
        "            match = re.search(regex, value)\n",
        "            if match:\n",
        "                key = match.group(\"key\")\n",
        "                start, end = match.span()\n",
        "\n",
        "                # Cerca la variabile esterna nel dizionario\n",
        "                for external_key, external_info in self.external_references.items():\n",
        "                    if key.startswith(external_key):\n",
        "                        # Estrae il valore reale della chiave\n",
        "                        actual_key = key[len(external_key) + 1:]\n",
        "                        tmp_value = value[:start] + external_info[\"logic\"](actual_key) + value[end:]  \n",
        "                        return tmp_value\n",
        "            return value\n",
        "\n",
        "        def resolve_list(l: List[Any]) -> None:\n",
        "            for index, item in enumerate(l):\n",
        "                if isinstance(item, dict):\n",
        "                    resolve_dict(item)\n",
        "                elif isinstance(item, str):\n",
        "                    l[index] = resolve_value(item)\n",
        "\n",
        "        def resolve_dict(d: Dict[str, Any]) -> None:\n",
        "            for k, v in d.items():\n",
        "                if isinstance(v, dict):\n",
        "                    resolve_dict(v)\n",
        "                elif isinstance(v, str):\n",
        "                    d[k] = resolve_value(v)\n",
        "                elif isinstance(v, list):\n",
        "                    resolve_list(v)\n",
        "\n",
        "        resolve_dict(yaml_data)\n",
        "        return yaml_data\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "  \n",
        "     \n",
        "\n",
        "    def _load_and_resolve_yaml_config(self) -> Dict[str, Any]:\n",
        "        # Carica il contenuto YAML come prima\n",
        "        yaml_data = self._load_yaml_config()\n",
        "        #sql_csv_path=\"abfss://dev@cfoadaadslgen2dlcert.dfs.core.windows.net/landing/prj_idqs/dq_control_results_test/file-sql-test.csv\"\n",
        "\n",
        "        df_from_sql_table =  self.sql_config_df\n",
        "        \n",
        "\n",
        "        # Caricare l'YAML in un dizionario\n",
        "\n",
        "        # Caricare il CSV in un DataFrame\n",
        "        \n",
        "\n",
        "        # Iterare sul DataFrame\n",
        "        import pandas as pd  \n",
        "        import yaml  \n",
        "        \n",
        "         \n",
        "        \n",
        "        from pyspark.sql import SparkSession  \n",
        "          \n",
        "        \n",
        "  \n",
        "        # UPDATE DELLO YAML CON CONFIGURAZIONE SQL\n",
        "        for row in df_from_sql_table.collect():  \n",
        "            row_dict = row.asDict()  \n",
        "            #print(f\"Processing row in CSV...\")  \n",
        "            # Trova la configurazione corrispondente nell'YAML  \n",
        "            for dimension in yaml_data['data_quality_dimensions']: \n",
        "                #print(f\"dimension:{yaml_data['data_quality_dimensions']}\")\n",
        "                for dq_rule in yaml_data['data_quality_dimensions'][dimension]:\n",
        "                    #print(f\"dq_rule:{dq_rule}\")   \n",
        "                    if dq_rule['name'] == row_dict['CONTROL_ID']:  \n",
        "                        #print(f\"Found matching rule {dq_rule['name']} in YAML...\")  \n",
        "                        # Aggiorna i campi\n",
        "                        dq_rule['enabled'] = False\n",
        "                        if row_dict['FLG_ENABLE_CONTROL'].lower() == 'y' or row_dict['FLG_ENABLE_CONTROL'].lower() == 's':  \n",
        "                            #print(\"Setting rule as enabled...\")  \n",
        "                            dq_rule['enabled'] = True  \n",
        "                        if row_dict['CONTROL_RESULTS'] == 'BLOCKING':  \n",
        "                            #print(\"Rule is blocking, updating blocking expectations...\")  \n",
        "                            blocking_expectation = '#{data_quality_dimensions:'+ dimension +':' + row_dict['CONTROL_ID'] + '}'  \n",
        "                            for policy in yaml_data['blocking_expectations_policy']:\n",
        "                                #controllo che non ci sia già prima di aggiungerla alla lista  \n",
        "                                if 'blocking_expectations' in policy and policy['blocking_expectations'] is not None:\n",
        "                                    if blocking_expectation not in policy['blocking_expectations'] and policy['policy_name']==row_dict['CONTROL_RESULTS']:  \n",
        "                                        #print(f\"Adding {blocking_expectation} to blocking expectations for policy {policy['policy_name']}...\")  \n",
        "                                        policy['blocking_expectations'].append(blocking_expectation)  \n",
        "                                    \n",
        "        print(\"Done processing CSV.\")\n",
        "  \n",
        "                \n",
        "\n",
        "        self.yaml_data=yaml_data\n",
        "\n",
        "        if self.external_references is not None:\n",
        "            # Risolve i riferimenti esterni\n",
        "            resolved_yaml_data = self._resolve_external_references(yaml_data)\n",
        "            \n",
        "            return resolved_yaml_data\n",
        "        else:\n",
        "            return yaml_data\n",
        "    import logging\n",
        "\n",
        "    \n",
        "\n",
        "    def _merge_dimension_and_catalog_expectations(self, dimension_expectation: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Combines an expectation from the dimension with its definition in the catalog.\n",
        "\n",
        "        This method merges an expectation defined in a data quality dimension\n",
        "        with its corresponding definition in the expectations catalog. It allows for\n",
        "        overriding default expectation settings with dimension-specific ones.\n",
        "\n",
        "        :param dimension_expectation: The expectation information from the data quality dimension.\n",
        "        :return: A combined expectation dictionary.\n",
        "        \"\"\"\n",
        "        if self.TAG_REFERENCE in dimension_expectation:\n",
        "            reference = dimension_expectation[self.TAG_REFERENCE]\n",
        "\n",
        "            # Utilizza la costante TAG_EXPECTATIONS_CATALOG per la sintassi #{expectations_catalog:<NOME CATALOGO>:<EXPERTATION NAME>}\n",
        "            #match = re.match(rf'#\\{{{self.TAG_EXPECTATIONS_CATALOG}:([^:]+):([^}]+)\\}}', reference)\n",
        "            #match = re.match(rf'#{{{{self.TAG_EXPECTATIONS_CATALOG}}}}:([^:]+):([^}]+)', reference)\n",
        "            #match = re.match(rf'#\\{{self.TAG_EXPECTATIONS_CATALOG}}:([^:]+):([^}]+)', reference)\n",
        "            #match = re.match(rf'#\\{{{{self.TAG_EXPECTATIONS_CATALOG}}}:([^:]+):([^}]+)', reference)\n",
        "            match = re.match(rf'#\\{{{self.TAG_EXPECTATIONS_CATALOG}}}:([^:]+):([^}}]+)', reference)\n",
        "\n",
        "\n",
        "            if match:\n",
        "                catalog_name, expectation_name = match.groups()\n",
        "                catalog_expectations = self.expectations_catalog.get(catalog_name, {}).get(TAG_EXPECTATIONS, [])\n",
        "                catalog_expectation = next((exp for exp in catalog_expectations if exp[self.TAG_NAME] == expectation_name), None)\n",
        "\n",
        "                if not catalog_expectation:\n",
        "                    raise ValueError(f\"Expectation {expectation_name} not found in catalog {catalog_name}.\")\n",
        "\n",
        "                combined_expectation = {**catalog_expectation, **dimension_expectation}\n",
        "            else:\n",
        "                raise ValueError(f\"Invalid reference format in dimension_expectation: {reference}\")\n",
        "        else:\n",
        "            combined_expectation = dimension_expectation\n",
        "\n",
        "        return combined_expectation\n",
        "\n",
        "    def _resolve_expectation_syntax(self, expectation_ref: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Resolves the syntax of an expectation reference from the YAML configuration.\n",
        "\n",
        "        This method interprets the custom syntax used in the YAML file to reference expectations.\n",
        "        The expectation references are expected to be in the format \"#{...}\".\n",
        "\n",
        "        Args:\n",
        "            expectation_ref (str): The expectation reference string in custom syntax.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Any]: A dictionary representing the resolved expectation details.\n",
        "        \"\"\"\n",
        "\n",
        "        # Regular expression to parse the custom syntax\n",
        "        pattern = r'\\#\\{([^}]+)\\}'\n",
        "        match = re.match(pattern, expectation_ref)\n",
        "\n",
        "        if match:\n",
        "            ref_parts = match.group(1).split(':')\n",
        "            \n",
        "            if len(ref_parts) == 3 and ref_parts[0] == self.TAG_DATA_QUALITY_DIMENSIONS:\n",
        "                dimension_name, expectation_name = ref_parts[1], ref_parts[2]\n",
        "                dimension_expectations = self.config_data[self.TAG_DATA_QUALITY_DIMENSIONS].get(dimension_name, [])\n",
        "                expectation = next((exp for exp in dimension_expectations if exp['name'] == expectation_name), None)\n",
        "\n",
        "                if expectation:\n",
        "                    return expectation\n",
        "                else:\n",
        "                    raise ValueError(f\"Expectation {expectation_name} not found in dimension {dimension_name}.\")\n",
        "            else:\n",
        "                raise ValueError(f\"Invalid reference format: {expectation_ref}\")\n",
        "        else:\n",
        "            raise ValueError(f\"Expectation reference does not match expected pattern: {expectation_ref}\")\n",
        "\n",
        "        return {}\n",
        "\n",
        "    def _get_blocking_expectations_info(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Extracts blocking expectations information from the YAML configuration file.\n",
        "\n",
        "        This method parses the 'blocking_expectations_policy' section in the YAML file and\n",
        "        retrieves details about each policy including its name, whether to stop on the first failure,\n",
        "        error handling strategy, fallback procedure, and the list of expectations.\n",
        "\n",
        "        :return: A dictionary where each key is a policy name and the value is another dictionary\n",
        "                containing details of the policy and its expectations.\n",
        "        \"\"\"\n",
        "        blocking_policies = self.config_data.get(self.TAG_BLOCKING_EXPECTATIONS_POLICY, [])\n",
        "        blocking_expectations_info = {}\n",
        "        #print(f\"blocking_policies:{blocking_policies}\")\n",
        "        for policy in blocking_policies:\n",
        "            policy_name = policy.get('policy_name')\n",
        "            stop_on_first_failure = policy.get('stop_on_first_failure', False)\n",
        "            error_handling_type = policy.get(self.TAG_ERROR_HANDLING)\n",
        "            fallback_procedure_type = policy.get(self.TAG_FALLBACK_PROCEDURE)\n",
        "            expectations_refs = policy.get(self.TAG_BLOCKING_EXPECTATIONS, [])\n",
        "            execute_all_expectations_then_halt_if_failed  = policy.get(self.TAG_HALT_IF_FAILED, False)\n",
        "            #print(f\"policy:{policy}\")\n",
        "\n",
        "            # Crea istanze delle classi ErrorHandler e FallbackProcedure\n",
        "            error_handling = ErrorHandler.get_instance(error_handling_type) if error_handling_type else None\n",
        "            fallback_procedure = FallbackProcedure.get_instance(fallback_procedure_type) if fallback_procedure_type else None\n",
        "\n",
        "            policy_info = {\n",
        "                'policy_name': policy_name,\n",
        "                'stop_on_first_failure': stop_on_first_failure,\n",
        "                'execute_all_expectations_then_halt_if_failed':execute_all_expectations_then_halt_if_failed,\n",
        "                'error_handling': error_handling,\n",
        "                'fallback_procedure': fallback_procedure,\n",
        "                'expectations': []\n",
        "            }\n",
        "            #if expectations_refs is not None:\n",
        "            for exp_ref in expectations_refs:\n",
        "                #print(f\"DQMANAGER:_get_blocking_expectations_info expectations_refs:{expectations_refs}, exp_ref:{exp_ref} \")\n",
        "                # Extract and combine expectations from the YAML configuration\n",
        "                # Replace the following line with your existing logic to handle exp_ref\n",
        "                expectation = self._resolve_expectation_syntax(exp_ref)\n",
        "                \n",
        "                if expectation:\n",
        "                    # Is used to combine the blocking expectations specified in the blocking expectations policy with their definitions in the expectations catalogue. \n",
        "                    # This method is essential to ensure that the blocking expectations have all the necessary information before being applied.\n",
        "                    combined_expectation = self._merge_dimension_and_catalog_expectations(expectation)\n",
        "                    policy_info['expectations'].append(combined_expectation)\n",
        "                else:\n",
        "                    print(f\"DqManager._get_blocking_expectations_info expectation not found:{expectation}\")\n",
        "\n",
        "            blocking_expectations_info[policy_name] = policy_info\n",
        "\n",
        "        return blocking_expectations_info\n",
        "    def _load_transformations(self):\n",
        "        \"\"\"Carica definizioni per \"data_transformation_and_distribution\" dal file YAML.\n",
        "        \"\"\"\n",
        "        transformations = self.config_data.get('data_transformation_and_distribution', [])\n",
        "        #print(transformations)\n",
        "        if transformations == []:\n",
        "            raise ValueError(\"Nessuna logica di traformazione inserita nello yaml, controllare presenza tag data_transformation_and_distribution\")\n",
        "        return transformations\n",
        "    \n",
        "    def apply_transformations(self, spark_df: DataFrame, *args, **kwargs) -> DataFrame: \n",
        "        # ... (Logica pre-esistente per gestione 'expectations') \n",
        "        \n",
        "        # ... la logica della trasformazione potrebbe essere gestita dopo la logica di 'expectations' o in metodo separato\n",
        "        transformations = self._load_transformations()\n",
        "        df_list = []\n",
        "        path_list = []\n",
        "        for transformation_def in transformations:\n",
        "            rule_name = transformation_def[\"rule_name\"].lower()\n",
        "            transformer_cls = FlowAdapterMeta.get_adapter(rule_name) \n",
        "            if not transformer_cls:\n",
        "                raise ValueError(f\"Transformer class for rule '{rule_name}' not found.\")\n",
        "\n",
        "            replaced_name = transformation_def.get(\"naming_convention\",None)\n",
        "            if(replaced_name):\n",
        "                input_path_dynamic = self.input_path_dynamic.split(\"-\")\n",
        "                input_path_dynamic[0] = replaced_name\n",
        "                self.input_path_dynamic = '-'.join(input_path_dynamic)\n",
        "                self.kwargs['input_path_dynamic']=self.input_path_dynamic\n",
        "            transformer = transformer_cls(data_sources=self.data_sources)  # Crea istanza del transformer di regola specifico\n",
        "            df,path = transformer.process_data(df =spark_df, \n",
        "                                    column_selection=transformation_def['column_selection'],\n",
        "                                    column_enrichment=transformation_def['column_enrichment'], \n",
        "                                    write_options = transformation_def.get('write_options', {}),\n",
        "                                    external_data = transformation_def.get('external_data', {}),\n",
        "                                    *self.args, \n",
        "                                    **self.kwargs\n",
        "                                    )\n",
        "                     \n",
        "            # write_options\n",
        "            df_list.append(df)\n",
        "            path_list.append(path.split(\".net\")[1])\n",
        "        return  df_list, path_list\n",
        "\n",
        "    def apply_expectations(self, spark_df: DataFrame, *args, **kwargs) -> DataFrame: \n",
        "        # Estrai informazioni sulle aspettative bloccanti\n",
        "        blocking_expectations_info = self._get_blocking_expectations_info()\n",
        "        \n",
        "        # Scegli la strategia in base alla presenza di aspettative bloccanti\n",
        "        if blocking_expectations_info:\n",
        "            strategy = BlockingFirstExecutionStrategy(policy_info=blocking_expectations_info)\n",
        "        else:\n",
        "            strategy = ExecutionStrategy.get_instance(self.STRATEGY_TYPE)\n",
        "\n",
        "        info_df = InfoDataframeDict(spark_df, {})\n",
        "        validator = SparkDataFrameValidator(info_df, data_sources=self.data_sources, args = self.args, kwargs = self.kwargs)\n",
        "        all_results_list = []\n",
        "        combined_expectations = {} # Dict[Dimension, List[Dict]] contiene la lista delle expertation e tutti gli attributi per ogni dimensione\n",
        "        \n",
        "        validator = SparkDataFrameValidator(info_df, data_sources=self.data_sources, args = self.args, kwargs = self.kwargs)\n",
        "\n",
        "        for dimension_name, dimension_expectations in self.config_data[self.TAG_DATA_QUALITY_DIMENSIONS].items():\n",
        "            dimension = Dimension.get_instance(dimension_name)\n",
        "            combined_expectations[dimension]=[]\n",
        "            for expectation in dimension_expectations:\n",
        "                # It is used to combine the expectation information specified in the dimension configuration with that defined in the expectations catalogue. \n",
        "                # This ensures that each applied expectation has the dimension-specific settings, overriding the default settings in the catalogue if necessary.\n",
        "                if expectation['enabled'] == True:\n",
        "                    combined_expectation = self._merge_dimension_and_catalog_expectations(expectation)\n",
        "                    combined_expectations[dimension].append(combined_expectation)\n",
        "                elif expectation['enabled'] == False :\n",
        "                    \n",
        "                    logger.info(f\"Expectation {expectation['name']} is disabled\")\n",
        "                    #print(f\"Expectation {expectation['name']}: is disabled {expectation} \")\n",
        "\n",
        "        # print(\"Parameters:\")\n",
        "        # pprint.pprint({\n",
        "        #     \"combined_expectations\": (type(combined_expectations), combined_expectations),\n",
        "        #     \"dimension\": (type(dimension), dimension),\n",
        "        #     \"validator\": (type(validator), validator),\n",
        "        #     \"spark_df\": (type(validator.get_df()), validator.get_df()),\n",
        "        # })\n",
        "        all_results, failed_controls = strategy.execute_expectations(dimension_expectations=combined_expectations,spark_data_frame_validator= validator)\n",
        "        all_results_list.append(all_results)\n",
        "\n",
        "        return validator,all_results_list , failed_controls"
      ]
    }
  ]
}