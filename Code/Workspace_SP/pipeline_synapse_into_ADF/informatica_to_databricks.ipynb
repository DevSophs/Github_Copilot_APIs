{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43e1684",
   "metadata": {},
   "source": [
    "# Informatica PowerCenter to Databricks Transformation\n",
    "\n",
    "This notebook reads the PowerCenter XML mapping definition and creates equivalent transformation logic in PySpark.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The original mapping (`mp_stg_TRANSACTION_CAMS_DD`) extracts data from TBHRD_TRANSACTION_CAMS_DD and loads it to TBST2_TRANSACTION_CAMS_DD with multiple column transformations. Key operations include:\n",
    "\n",
    "1. Reading from Oracle source table\n",
    "2. Data type conversions\n",
    "3. String trimming\n",
    "4. Date format conversions\n",
    "5. NULL handling\n",
    "6. Data masking for sensitive data\n",
    "7. Writing to target table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e6871d",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "First, we'll import necessary libraries and define configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab79d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "# Configuration parameters\n",
    "source_table = \"TBHRD_TRANSACTION_CAMS_DD\"\n",
    "target_table = \"TBST2_TRANSACTION_CAMS_DD\"\n",
    "source_database = \"ODS_Sviluppo\"\n",
    "source_schema = \"HRD_CAMS_OBJ\"\n",
    "\n",
    "# Date parameters (will be passed as parameters in ADF)\n",
    "p_dt_inf = spark.conf.get(\"spark.databricks.job.p_dt_inf\", \"2025-03-01 00:00:00\")\n",
    "p_dt_sup = spark.conf.get(\"spark.databricks.job.p_dt_sup\", \"2025-03-21 23:59:59\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85978f2d",
   "metadata": {},
   "source": [
    "## 2. JDBC Connection Setup\n",
    "\n",
    "Configure JDBC connection for Oracle source database. In a production environment, secrets would be retrieved from Azure Key Vault."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JDBC connection configuration\n",
    "jdbc_url = f\"jdbc:oracle:thin:@//your-oracle-server:1521/{source_database}\"\n",
    "connection_properties = {\n",
    "    \"user\": \"dbutils.secrets.get(scope='oracle-keys', key='username')\",\n",
    "    \"password\": \"dbutils.secrets.get(scope='oracle-keys', key='password')\",\n",
    "    \"driver\": \"oracle.jdbc.driver.OracleDriver\",\n",
    "    \"fetchsize\": \"10000\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fec4fe",
   "metadata": {},
   "source": [
    "## 3. Source Query Definition\n",
    "\n",
    "Define the source query based on the Informatica mapping's Source Qualifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build query similar to the SQL in Source Qualifier\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "  {source_table}.DATA_APP,\n",
    "  {source_table}.COMPANY_GROUPS,\n",
    "  {source_table}.AMBIENTE,\n",
    "  {source_table}.POOL,\n",
    "  {source_table}.COMPANY,\n",
    "  {source_table}.AC_GRP_INTRL_ID,\n",
    "  {source_table}.AC_INTRL_ID,\n",
    "  {source_table}.TX_CYC_BGN_DT,\n",
    "  {source_table}.TX_GU_EFF_DT,\n",
    "  {source_table}.JO_PSTG_SEQ_NR,\n",
    "  {source_table}.JO_INTRL_SEQ_NR,\n",
    "  {source_table}.JO_PSTG_CD,\n",
    "  {source_table}.JO_PSTG_ID,\n",
    "  standard_hash(TRIM({source_table}.JO_PSTG_ID), 'MD5') AS ID_CAR,\n",
    "  standard_hash(TRIM({source_table}.JO_PSTG_ID), 'SHA512') AS CO_HASHED_PAN,\n",
    "  SUBSTR(TRIM({source_table}.JO_PSTG_ID), 0, 6) || LPAD(SUBSTR(TRIM({source_table}.JO_PSTG_ID), LENGTH(TRIM({source_table}.JO_PSTG_ID))-3, LENGTH(TRIM({source_table}.JO_PSTG_ID))), LENGTH(SUBSTR(TRIM({source_table}.JO_PSTG_ID), 7, LENGTH(TRIM({source_table}.JO_PSTG_ID)))), '*') AS CO_CAR,\n",
    "  {source_table}.JO_DB_CR_IN,\n",
    "  {source_table}.GU_PSTG_DT,\n",
    "  {source_table}.JO_SRCE_CD,\n",
    "  {source_table}.JO_CATG_CD,\n",
    "  {source_table}.JO_LVL_3_CD,\n",
    "  {source_table}.JO_LVL_4_CD,\n",
    "  {source_table}.JO_LVL_5_CD,\n",
    "  {source_table}.JO_PSTD_AM,\n",
    "  {source_table}.JO_AVAIL_RV_AM,\n",
    "  {source_table}.JO_PRT_ON_SN_IN\n",
    "  -- ... remaining fields would be included here\n",
    "FROM {source_schema}.{source_table}\n",
    "WHERE TO_TIMESTAMP(TRIM(TC_UPDT_TS), 'YYYY-MM-DD-HH24.MI.SS.FF') >= TO_DATE('{p_dt_inf}', 'YYYY-MM-DD HH24:MI:SS') \n",
    "  AND TO_TIMESTAMP(TRIM(TC_UPDT_TS), 'YYYY-MM-DD-HH24.MI.SS.FF') <= TO_DATE('{p_dt_sup}', 'YYYY-MM-DD HH24:MI:SS')\n",
    "  AND (TRIM(TC_UPDT_TS) = TRIM(TC_CRT_TS))\n",
    "\"\"\"\n",
    "\n",
    "print(\"Source query defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa5ba47",
   "metadata": {},
   "source": [
    "## 4. Custom Functions for Transformations\n",
    "\n",
    "Define custom utility functions to simulate Informatica's transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register UDFs for transformations similar to Informatica's Expression transformation\n",
    "\n",
    "# Function to handle date conversion with format validation\n",
    "@F.udf(returnType=TimestampType())\n",
    "def safe_to_date(date_str, format_str):\n",
    "    \"\"\"Convert string to date with null handling\"\"\"\n",
    "    if date_str is None or date_str.strip() == \"\":\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.strptime(date_str.strip(), format_str)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Function to replicate standard_hash in Oracle for MD5\n",
    "@F.udf(returnType=StringType())\n",
    "def md5_hash(text):\n",
    "    \"\"\"Generate MD5 hash similar to Oracle's standard_hash\"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    return hashlib.md5(text.encode()).hexdigest()\n",
    "\n",
    "# Function to replicate standard_hash in Oracle for SHA512\n",
    "@F.udf(returnType=StringType())\n",
    "def sha512_hash(text):\n",
    "    \"\"\"Generate SHA512 hash similar to Oracle's standard_hash\"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    return hashlib.sha512(text.encode()).hexdigest()\n",
    "\n",
    "# Function to mask PAN numbers (keep first 6 and last 4 digits)\n",
    "@F.udf(returnType=StringType())\n",
    "def mask_pan(pan):\n",
    "    \"\"\"Mask PAN number by keeping first 6 and last 4 digits\"\"\"\n",
    "    if pan is None or len(pan.strip()) == 0:\n",
    "        return None\n",
    "    pan = pan.strip()\n",
    "    if len(pan) <= 10:  # Not enough to mask meaningfully\n",
    "        return pan\n",
    "    prefix = pan[:6]\n",
    "    suffix = pan[-4:]\n",
    "    mask_len = len(pan) - 10\n",
    "    return prefix + '*' * mask_len + suffix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91a907e",
   "metadata": {},
   "source": [
    "## 5. Data Extraction and Transformation\n",
    "\n",
    "Read and transform the data from source to target format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae431577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data using the query\n",
    "df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", f\"({query})\") \\\n",
    "    .option(\"user\", connection_properties[\"user\"]) \\\n",
    "    .option(\"password\", connection_properties[\"password\"]) \\\n",
    "    .option(\"driver\", connection_properties[\"driver\"]) \\\n",
    "    .option(\"fetchsize\", connection_properties[\"fetchsize\"]) \\\n",
    "    .load()\n",
    "\n",
    "print(f\"Retrieved {df.count()} rows from source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb2d7b",
   "metadata": {},
   "source": [
    "## 6. Apply Transformations\n",
    "\n",
    "Apply transformations equivalent to the Informatica Expression transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d88b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations to match Informatica expressions\n",
    "transformed_df = df.withColumn(\"DATA_APP\", F.trim(F.col(\"DATA_APP\"))) \\\n",
    "    .withColumn(\"COMPANY_GROUPS\", F.trim(F.col(\"COMPANY_GROUPS\"))) \\\n",
    "    .withColumn(\"AMBIENTE\", F.trim(F.col(\"AMBIENTE\"))) \\\n",
    "    .withColumn(\"POOL\", F.trim(F.col(\"POOL\"))) \\\n",
    "    .withColumn(\"COMPANY\", F.trim(F.col(\"COMPANY\"))) \\\n",
    "    .withColumn(\"AC_GRP_INTRL_ID\", F.col(\"AC_GRP_INTRL_ID\").cast(\"decimal(13,0)\")) \\\n",
    "    .withColumn(\"AC_INTRL_ID\", F.col(\"AC_INTRL_ID\").cast(\"decimal(13,0)\")) \\\n",
    "    .withColumn(\"TX_CYC_BGN_DT\", safe_to_date(F.col(\"TX_CYC_BGN_DT\"), \"%Y-%m-%d\")) \\\n",
    "    .withColumn(\"TX_GU_EFF_DT\", safe_to_date(F.col(\"TX_GU_EFF_DT\"), \"%Y-%m-%d\")) \\\n",
    "    .withColumn(\"JO_PSTG_SEQ_NR\", F.col(\"JO_PSTG_SEQ_NR\").cast(\"decimal(5,0)\")) \\\n",
    "    .withColumn(\"JO_INTRL_SEQ_NR\", F.col(\"JO_INTRL_SEQ_NR\").cast(\"decimal(5,0)\")) \\\n",
    "    .withColumn(\"JO_PSTG_CD\", F.trim(F.col(\"JO_PSTG_CD\"))) \\\n",
    "    .withColumn(\"JO_DB_CR_IN\", F.trim(F.col(\"JO_DB_CR_IN\"))) \\\n",
    "    .withColumn(\"GU_PSTG_DT\", safe_to_date(F.col(\"GU_PSTG_DT\"), \"%Y-%m-%d\")) \\\n",
    "    .withColumn(\"JO_SRCE_CD\", F.trim(F.col(\"JO_SRCE_CD\"))) \\\n",
    "    .withColumn(\"JO_CATG_CD\", F.trim(F.col(\"JO_CATG_CD\"))) \\\n",
    "    .withColumn(\"JO_LVL_3_CD\", F.trim(F.col(\"JO_LVL_3_CD\"))) \\\n",
    "    .withColumn(\"JO_LVL_4_CD\", F.trim(F.col(\"JO_LVL_4_CD\"))) \\\n",
    "    .withColumn(\"JO_LVL_5_CD\", F.trim(F.col(\"JO_LVL_5_CD\"))) \\\n",
    "    .withColumn(\"JO_PSTD_AM\", F.col(\"JO_PSTD_AM\").cast(\"decimal(15,2)\")) \\\n",
    "    .withColumn(\"JO_AVAIL_RV_AM\", F.col(\"JO_AVAIL_RV_AM\").cast(\"decimal(15,2)\")) \\\n",
    "    .withColumn(\"JO_PRT_ON_SN_IN\", F.when(F.col(\"JO_PRT_ON_SN_IN\").isNull() | (F.trim(F.col(\"JO_PRT_ON_SN_IN\")) == \"\"), F.lit(\"N\")).otherwise(F.trim(F.col(\"JO_PRT_ON_SN_IN\"))))\n",
    "    # ... additional transformations would be applied for remaining fields\n",
    "\n",
    "# Rename columns to match target schema if needed\n",
    "transformed_df = transformed_df.withColumnRenamed(\"AMBIENTE\", \"ENVIROMENT\")\n",
    "transformed_df = transformed_df.withColumnRenamed(\"COMPANY\", \"AC_CO_NR\")\n",
    "transformed_df = transformed_df.withColumnRenamed(\"TX_CYC_BGN_DT\", \"SN_CYC_BGN_DT\")\n",
    "transformed_df = transformed_df.withColumnRenamed(\"TX_GU_EFF_DT\", \"GU_EFF_DT\")\n",
    "\n",
    "print(\"Transformations applied successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76c4068",
   "metadata": {},
   "source": [
    "## 7. Write Data to Target\n",
    "\n",
    "Write the transformed data to the target table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c3895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to target table\n",
    "target_jdbc_url = f\"jdbc:oracle:thin:@//your-oracle-server:1521/{source_database}\"\n",
    "\n",
    "transformed_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", target_jdbc_url) \\\n",
    "    .option(\"dbtable\", target_table) \\\n",
    "    .option(\"user\", connection_properties[\"user\"]) \\\n",
    "    .option(\"password\", connection_properties[\"password\"]) \\\n",
    "    .option(\"driver\", connection_properties[\"driver\"]) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "print(f\"Data successfully written to {target_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e169e",
   "metadata": {},
   "source": [
    "## 8. XML Parsing Utility\n",
    "\n",
    "This utility function can parse the Informatica XML directly to generate transformation logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0d9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_informatica_xml(xml_path):\n",
    "    \"\"\"Parse Informatica PowerCenter XML to extract mapping information\"\"\"\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Find the mapping\n",
    "    mapping = root.find(\".//MAPPING\")\n",
    "    mapping_name = mapping.get(\"NAME\")\n",
    "    print(f\"Found mapping: {mapping_name}\")\n",
    "    \n",
    "    # Extract source and target information\n",
    "    source = root.find(\".//SOURCE\")\n",
    "    source_name = source.get(\"NAME\")\n",
    "    source_db = source.get(\"DBDNAME\")\n",
    "    source_owner = source.get(\"OWNERNAME\")\n",
    "    \n",
    "    target = root.find(\".//TARGET\")\n",
    "    target_name = target.get(\"NAME\")\n",
    "    \n",
    "    # Get source fields\n",
    "    source_fields = []\n",
    "    for field in source.findall(\"./SOURCEFIELD\"):\n",
    "        source_fields.append({\n",
    "            \"name\": field.get(\"NAME\"),\n",
    "            \"datatype\": field.get(\"DATATYPE\"),\n",
    "            \"precision\": field.get(\"PRECISION\"),\n",
    "            \"scale\": field.get(\"SCALE\"),\n",
    "            \"keytype\": field.get(\"KEYTYPE\")\n",
    "        })\n",
    "    \n",
    "    # Get target fields\n",
    "    target_fields = []\n",
    "    for field in target.findall(\"./TARGETFIELD\"):\n",
    "        target_fields.append({\n",
    "            \"name\": field.get(\"NAME\"),\n",
    "            \"datatype\": field.get(\"DATATYPE\"),\n",
    "            \"precision\": field.get(\"PRECISION\"),\n",
    "            \"scale\": field.get(\"SCALE\"),\n",
    "            \"keytype\": field.get(\"KEYTYPE\")\n",
    "        })\n",
    "    \n",
    "    # Find transformation expressions\n",
    "    expr_trans = root.find(\".//TRANSFORMATION[@TYPE='Expression']\")\n",
    "    expressions = []\n",
    "    \n",
    "    if expr_trans is not None:\n",
    "        for field in expr_trans.findall(\"./TRANSFORMFIELD[@PORTTYPE='OUTPUT']\"):\n",
    "            expr = field.get(\"EXPRESSION\")\n",
    "            if expr:\n",
    "                expressions.append({\n",
    "                    \"name\": field.get(\"NAME\"),\n",
    "                    \"expression\": expr,\n",
    "                    \"datatype\": field.get(\"DATATYPE\")\n",
    "                })\n",
    "    \n",
    "    return {\n",
    "        \"mapping_name\": mapping_name,\n",
    "        \"source\": {\n",
    "            \"name\": source_name,\n",
    "            \"database\": source_db,\n",
    "            \"owner\": source_owner,\n",
    "            \"fields\": source_fields\n",
    "        },\n",
    "        \"target\": {\n",
    "            \"name\": target_name,\n",
    "            \"fields\": target_fields\n",
    "        },\n",
    "        \"expressions\": expressions\n",
    "    }\n",
    "\n",
    "# Example usage (commented out as XML file not available in notebook context)\n",
    "# mapping_info = parse_informatica_xml(\"prova_script_IPWC.XML\")\n",
    "# print(f\"Parsed {len(mapping_info['source']['fields'])} source fields\")\n",
    "# print(f\"Parsed {len(mapping_info['target']['fields'])} target fields\")\n",
    "# print(f\"Parsed {len(mapping_info['expressions'])} expressions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55c5ad",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "This notebook demonstrates how to implement Informatica PowerCenter mapping logic in Databricks using PySpark.\n",
    "\n",
    "Key benefits of this approach:\n",
    "\n",
    "1. **Scalability**: PySpark can handle larger volumes of data than traditional ETL tools\n",
    "2. **Cost Efficiency**: Uses cloud-native processing rather than dedicated ETL servers\n",
    "3. **Flexibility**: Can be integrated with modern data platforms and processing frameworks\n",
    "4. **Maintainability**: Code-based approach allows for version control and CI/CD integration\n",
    "5. **Extensibility**: Easy to add additional transformations and business logic"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
