{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Informatica PowerCenter XML to Databricks Notebook\n",
    "I'll help you convert this Informatica PowerCenter XML file to a Databricks notebook. Based on the XML content, this appears to be an ETL process that deals with credit card transaction data (CAMS - Card Account Management System).\n",
    "\n",
    "Here's how you can create a Databricks notebook equivalent:\n",
    "\n",
    "1. Create a new Databricks notebook\n",
    "First, create a new notebook in your Databricks workspace.\n",
    "\n",
    "2. Set up notebook structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook equivalent of Informatica PowerCenter mapping: mp_mkt_MOVIMENTI_CARTA_CAMS_DD\n",
    "# Original workflow: wf_stg_TRANSACTION_CAMS_DD\n",
    "# Created: April 1, 2025\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Configuration Parameters\n",
    "# These would replace the Informatica mapping parameters\n",
    "source_table = \"TBHRD_TRANSACTION_CAMS_DD\"\n",
    "target_table = \"TBMK2_MOVIMENTI_CARTA_CAMS_DD\"\n",
    "lookup_table_trx_cd = \"TBMK2_TRX_CD_CAMS\"\n",
    "lookup_table_grp_cmp = \"TBMK2_GRP_CMP\"\n",
    "lookup_table_ac_cmpy = \"TBMK2_AC_CMPY_OWN_LVL_CAMS\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Data source connection\n",
    "# Replace with your actual connection details\n",
    "def get_source_data():\n",
    "    \"\"\"Read source data - equivalent to Source Qualifier in Informatica\"\"\"\n",
    "    \n",
    "    # Read the source table\n",
    "    df_source = spark.table(source_table)\n",
    "    \n",
    "    print(f\"Source data loaded: {df_source.count()} rows\")\n",
    "    return df_source\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create lookup functions - equivalent to Lookup transformations in Informatica\n",
    "def lookup_trx_cd_cams(df):\n",
    "    \"\"\"\n",
    "    Implements the LKP_TBMK2_TRX_CD_CAMS lookup transformation\n",
    "    \"\"\"\n",
    "    lookup_df = spark.table(lookup_table_trx_cd)\n",
    "    \n",
    "    # Cache the lookup table for better performance\n",
    "    lookup_df.cache()\n",
    "    \n",
    "    # Register as temp view for SQL operations if needed\n",
    "    lookup_df.createOrReplaceTempView(\"lkp_trx_cd_cams\")\n",
    "    \n",
    "    # Join with main dataframe based on lookup condition\n",
    "    # JO_UCS_TXN_CD = IN_JO_UCS_TXN_CD AND AC_CO_NR = IN_AC_CO_NR\n",
    "    joined_df = df.join(\n",
    "        lookup_df,\n",
    "        (df[\"JO_UCS_TXN_CD\"] == lookup_df[\"JO_UCS_TXN_CD\"]) & \n",
    "        (df[\"AC_CO_NR\"] == lookup_df[\"AC_CO_NR\"]),\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    return joined_df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def lookup_grp_cmp(df):\n",
    "    \"\"\"\n",
    "    Implements the LKP2_TBMK2_GRP_CMP lookup transformation\n",
    "    \"\"\"\n",
    "    lookup_df = spark.table(lookup_table_grp_cmp)\n",
    "    \n",
    "    # Cache the lookup table for better performance\n",
    "    lookup_df.cache()\n",
    "    \n",
    "    # Join with main dataframe based on lookup condition\n",
    "    # CO_CMPY = IN_CO_CMPY\n",
    "    joined_df = df.join(\n",
    "        lookup_df,\n",
    "        df[\"CO_CMPY\"] == lookup_df[\"CO_CMPY\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    return joined_df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def lookup_ac_cmpy_own_lvl(df):\n",
    "    \"\"\"\n",
    "    Implements the LKP_TBMK2_AC_CMPY_OWN_LVL_CAMS lookup transformation\n",
    "    \"\"\"\n",
    "    lookup_df = spark.table(lookup_table_ac_cmpy)\n",
    "    \n",
    "    # Cache the lookup table for better performance\n",
    "    lookup_df.cache()\n",
    "    \n",
    "    # Join with main dataframe based on lookup condition\n",
    "    # AC_CO_NR = IN_AC_CO_NR\n",
    "    joined_df = df.join(\n",
    "        lookup_df,\n",
    "        df[\"AC_CO_NR\"] == lookup_df[\"AC_CO_NR\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    return joined_df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def expression_transform(df):\n",
    "    \"\"\"\n",
    "    Implements the EXPTRANS2 expression transformation\n",
    "    Handling conditional logic similar to the Informatica expressions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Implement conditional expressions\n",
    "    # IIF(CO_CIR1='M',POS_EM_DE221,NULL)\n",
    "    df = df.withColumn(\"O_POS_EM_DE221_MC\", \n",
    "                      when(col(\"CO_CIR1\") == \"M\", col(\"POS_EM_DE22\")).otherwise(None))\n",
    "    \n",
    "    # IIF(CO_CIR1='V',POS_EM_DE221,NULL)\n",
    "    df = df.withColumn(\"O_POS_EM_DE221_VI\", \n",
    "                      when(col(\"CO_CIR1\") == \"V\", col(\"POS_EM_DE22\")).otherwise(None))\n",
    "    \n",
    "    # Implement additional EXPTRANS1 logic\n",
    "    df = df.withColumn(\"O_FL_CASH\", \n",
    "                      when((col(\"FL_CASH\").isNull()) | (trim(col(\"FL_CASH\")) == \"\"), \n",
    "                           lit(\"N\")).otherwise(trim(col(\"FL_CASH\"))))\n",
    "    \n",
    "    df = df.withColumn(\"O_FL_REVERSAL\", \n",
    "                      when((col(\"FL_REVERSAL\").isNull()) | (trim(col(\"FL_REVERSAL\")) == \"\"), \n",
    "                           lit(\"N\")).otherwise(trim(col(\"FL_REVERSAL\"))))\n",
    "\n",
    "    # Add the new logic for JO_LVL_3_CD\n",
    "    # IIF(JO_LVL_3_CD = 'DOM', 'S', 'N')\n",
    "    df = df.withColumn(\"O_JO_LVL_3_CD_FLAG\", \n",
    "                      when(col(\"JO_LVL_3_CD\") == \"DOM\", lit(\"S\")).otherwise(lit(\"N\")))\n",
    "\n",
    "    return df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main ETL process\"\"\"\n",
    "    \n",
    "    # Step 1: Read source data\n",
    "    df = get_source_data()\n",
    "    \n",
    "    # Step 2: Apply lookups\n",
    "    df = lookup_trx_cd_cams(df)\n",
    "    df = lookup_grp_cmp(df)\n",
    "    df = lookup_ac_cmpy_own_lvl(df)\n",
    "    \n",
    "    # Step 3: Apply expression transformations\n",
    "    df = expression_transform(df)\n",
    "    \n",
    "    # Step 4: Select and rename columns for target\n",
    "    # Map the columns from source to target based on the connector mappings in the XML\n",
    "    final_df = df.select(\n",
    "        col(\"ID_CMPY\"),\n",
    "        col(\"TX_CYC_BGN_DT\").alias(\"DT_INI_CICLO\"),\n",
    "        col(\"TR_DT\").alias(\"DT_MOV\"),\n",
    "        col(\"JO_PSTG_CD\").alias(\"CO_POST_CODE\"),\n",
    "        col(\"CO_CAR\"),\n",
    "        col(\"ID_CAR\"),\n",
    "        col(\"CO_HASHED_PAN\"),\n",
    "        col(\"GU_PSTG_DT\").alias(\"DT_CTBZ\"),\n",
    "        col(\"JO_LVL_3_CD\").alias(\"CO_TX_LEV3\"),\n",
    "        col(\"JO_LVL_4_CD\").alias(\"CO_TX_LEV4\"),\n",
    "        col(\"JO_LVL_5_CD\").alias(\"CO_TX_LEV5\"),\n",
    "        col(\"JO_PSTD_AM\").alias(\"VA_SPE_RIF\"),\n",
    "        col(\"JO_PRT_ON_SN_IN\").alias(\"FL_STATEMENT\"),\n",
    "        col(\"JO_UCS_TXN_CD\").alias(\"CO_TX_CODE\"),\n",
    "        current_timestamp().alias(\"DT_INS\"),\n",
    "        col(\"CO_RRN_ORIG\"),\n",
    "        col(\"JO_TXN_ORIG_AM\").alias(\"VA_SPE_ORI\"),\n",
    "        col(\"JO_TXN_CURR_CD\").alias(\"CO_DIV_RIF\"),\n",
    "        col(\"CO_SICC\"),\n",
    "        col(\"AU_A_DT\").alias(\"DT_AUTH\"),\n",
    "        col(\"AU_SEQ_NR\").alias(\"CO_PSEQNUM\"),\n",
    "        col(\"TX_USAGE_CODE\").alias(\"CO_USAGE_CODE\"),\n",
    "        col(\"TX_CHRGBK_REF_NBR\").alias(\"CHBK_REF_NBR\"),\n",
    "        col(\"DS_ACQ_ID\").alias(\"CO_ACQUIRER\"),\n",
    "        col(\"DS_AUTH_CODE\").alias(\"CO_AUTORIZZ\"),\n",
    "        col(\"DS_CRD_ACCPT_ID\").alias(\"CO_ESE_ACC\"),\n",
    "        col(\"DS_RECURRING_TXN_IN\").alias(\"FL_MOV_RIC\"),\n",
    "        col(\"NM_LOC_ACC\"),\n",
    "        col(\"TR_MRCH_CNTRY_CD\").alias(\"CO_NAZ_SP\"),\n",
    "        col(\"TX_MRCH_ZIP_CD\").alias(\"CO_ZIP\"),\n",
    "        col(\"DS_TXN_DESC_DA\").alias(\"TE_TXN_DESC_EC\"),\n",
    "        col(\"TX_SRC_CURR_CD\").alias(\"CO_DIV_ORI\"),\n",
    "        col(\"TX_DCC_FL\").alias(\"FL_DCC\"),\n",
    "        col(\"TX_INTCHANG_FEE_AM\").alias(\"VA_INT_FEE\"),\n",
    "        col(\"ACQ_REF_NR\").alias(\"CO_ARN\"),\n",
    "        col(\"CO_TP_MERCATO\"),\n",
    "        col(\"TRK_TOKEN_REQUESTOR_ID\").alias(\"CO_TRX_TOKEN_REQUESTOR_ID\"),\n",
    "        col(\"NM_NOME_ACC\"),\n",
    "        col(\"TRK_TOKEN\").alias(\"CO_TRX_TOKEN\"),\n",
    "        col(\"FL_INTERNET\").alias(\"FL_MOV_INTERNET\"),\n",
    "        col(\"FL_DIG_PAN_CIR\"),\n",
    "        col(\"FL_CP_CNP\"),\n",
    "        col(\"FL_MOTO\"),\n",
    "        col(\"FL_NFC\"),\n",
    "        col(\"CO_TP_ECOM_SICU\"),\n",
    "        col(\"TLID\").alias(\"ID_AUTH_MOV\"),\n",
    "        col(\"JDX_TOKEN\").alias(\"ID_MSG\"),\n",
    "        col(\"FL_CLESS\"),\n",
    "        col(\"FL_CHIP\"),\n",
    "        col(\"O_POS_EM_DE221_MC\").alias(\"CO_MC_DE22\"),\n",
    "        col(\"O_POS_EM_DE221_VI\").alias(\"CO_VI_POS_ENTRY_MODE\"),\n",
    "        col(\"CO_TP_MOV_CHIP\"),\n",
    "\n",
    "        # Create a unique ID for the target primary key without separators\n",
    "    final_df = final_df.withColumn(\n",
    "        \"ID_MOV\",\n",
    "        concat(\n",
    "            col(\"AC_CO_NR\"),\n",
    "            col(\"AC_INTRL_ID\"),\n",
    "            date_format(col(\"SN_CYC_BGN_DT\"), \"yyyyMMdd\"),\n",
    "            date_format(col(\"GU_EFF_DT\"), \"yyyyMMdd\"),\n",
    "            col(\"JO_PSTG_SEQ_NR\"),\n",
    "            col(\"JO_INTRL_SEQ_NR\"),\n",
    "            substring(date_format(col(\"TC_UPDT_TS\"), \"yyyy-MM-dd-HH24.MI.SS.US\"), 1, 19)\n",
    "        )\n",
    "    )\n",
    "\n",
    "        # Additional required fields\n",
    "        lit(\"CAM\").alias(\"CO_DB_PROVENIENZA\"),\n",
    "        col(\"CO_CIR1\").alias(\"ID_CIR\"),\n",
    "        col(\"O_FL_CASH\").alias(\"FL_CASH\"),\n",
    "        # Domestic flag logic\n",
    "        when(col(\"TR_MRCH_CNTRY_CD\") == \"380\", lit(\"Y\")).otherwise(lit(\"N\")).alias(\"FL_DOMESTIC\"),\n",
    "        # Type of operation - would need actual business logic\n",
    "        lit(1).cast(\"decimal(2,0)\").alias(\"ID_TP_OPE\"),\n",
    "        # Entity code - would need actual mapping\n",
    "        lit(null()).cast(\"string\").alias(\"CO_EN\"),\n",
    "        # Technology ID - would need actual mapping\n",
    "        lit(1).cast(\"decimal(2,0)\").alias(\"ID_TECNOLOGIA\"),\n",
    "        # Year/month - extract from date\n",
    "        date_format(col(\"TR_DT\"), \"yyMM\").cast(\"decimal(4,0)\").alias(\"ID_AAMM\")\n",
    "    )\n",
    "    \n",
    "    # Step 5: Write to target table\n",
    "    final_df.write.mode(\"append\").option(\"overwriteSchema\", \"true\").saveAsTable(target_table)\n",
    "    \n",
    "    print(f\"Data loaded to target: {final_df.count()} rows\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Execute the main ETL process\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "This notebook replicates the flow of Informatica PowerCenter mapping by:\n",
    "\n",
    "Reading from source tables\n",
    "Implementing lookups similar to LKP_TBMK2_TRX_CD_CAMS, LKP2_TBMK2_GRP_CMP, etc.\n",
    "Creating expression transformations similar to EXPTRANS2\n",
    "Mapping source to target columns based on connector definitions\n",
    "Key differences from Informatica PowerCenter:\n",
    "\n",
    "Uses Spark DataFrame API instead of Informatica's proprietary transformation logic\n",
    "Leverages Spark's built-in functions for data transformation\n",
    "Uses SQL joins for lookups instead of Informatica's Lookup transformation\n",
    "Data types are handled by Spark's type system\n",
    "Databricks-specific features:\n",
    "\n",
    "Uses Databricks tables for source and target\n",
    "Can be scheduled using Databricks Jobs\n",
    "Leverages Spark's distributed processing\n",
    "This notebook provides a starting point that you can customize based on your specific requirements and Databricks environment."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
