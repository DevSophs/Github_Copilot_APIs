{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment\n",
    "Import necessary libraries including PySpark, YAML parser, and other utilities needed for Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession  # For creating and managing Spark sessions\n",
    "import yaml  # For parsing YAML configuration files\n",
    "import os  # For handling file paths and environment variables\n",
    "import logging  # For logging purposes\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataQualityManager\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set logging level\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"DataQualityManager\")\n",
    "\n",
    "# Verify Spark session\n",
    "logger.info(\"Spark session initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define DataQualityManager Class\n",
    "Create the main DataQualityManager class with methods for configuration interpretation, execution coordination, and result management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DataQualityManager class\n",
    "class DataQualityManager:\n",
    "    def __init__(self, config_path):\n",
    "        \"\"\"\n",
    "        Initialize the DataQualityManager with the path to the YAML configuration file.\n",
    "        \"\"\"\n",
    "        self.config_path = config_path\n",
    "        self.config = None\n",
    "        self.execution_strategy = None\n",
    "        self.results = []\n",
    "\n",
    "    def load_config(self):\n",
    "        \"\"\"\n",
    "        Load and parse the YAML configuration file.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.config_path):\n",
    "            raise FileNotFoundError(f\"Configuration file not found: {self.config_path}\")\n",
    "        \n",
    "        with open(self.config_path, 'r') as file:\n",
    "            self.config = yaml.safe_load(file)\n",
    "        \n",
    "        logger.info(\"Configuration loaded successfully.\")\n",
    "\n",
    "    def set_execution_strategy(self, strategy):\n",
    "        \"\"\"\n",
    "        Set the execution strategy for applying data quality expectations.\n",
    "        \"\"\"\n",
    "        self.execution_strategy = strategy\n",
    "        logger.info(f\"Execution strategy set to: {strategy.__class__.__name__}\")\n",
    "\n",
    "    def apply_expectations(self, dataframe):\n",
    "        \"\"\"\n",
    "        Apply data quality expectations to the given Spark DataFrame using the configured strategy.\n",
    "        \"\"\"\n",
    "        if not self.execution_strategy:\n",
    "            raise ValueError(\"Execution strategy is not set.\")\n",
    "        \n",
    "        if not self.config or 'expectations' not in self.config:\n",
    "            raise ValueError(\"Expectations are not defined in the configuration.\")\n",
    "        \n",
    "        expectations = self.config['expectations']\n",
    "        logger.info(f\"Applying {len(expectations)} expectations to the DataFrame.\")\n",
    "        \n",
    "        self.results = self.execution_strategy.execute(dataframe, expectations)\n",
    "        logger.info(\"Expectations applied successfully.\")\n",
    "\n",
    "    def aggregate_results(self):\n",
    "        \"\"\"\n",
    "        Aggregate and process the results of the executed expectations.\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            raise ValueError(\"No results to aggregate.\")\n",
    "        \n",
    "        # Example aggregation logic: count passed and failed expectations\n",
    "        passed = sum(1 for result in self.results if result['status'] == 'passed')\n",
    "        failed = sum(1 for result in self.results if result['status'] == 'failed')\n",
    "        \n",
    "        logger.info(f\"Results aggregated: {passed} passed, {failed} failed.\")\n",
    "        return {\"passed\": passed, \"failed\": failed}\n",
    "\n",
    "    def handle_fallbacks(self):\n",
    "        \"\"\"\n",
    "        Handle fallback procedures as defined in the configuration.\n",
    "        \"\"\"\n",
    "        if 'fallbacks' not in self.config:\n",
    "            logger.info(\"No fallback procedures defined in the configuration.\")\n",
    "            return\n",
    "        \n",
    "        fallbacks = self.config['fallbacks']\n",
    "        logger.info(f\"Executing {len(fallbacks)} fallback procedures.\")\n",
    "        \n",
    "        for fallback in fallbacks:\n",
    "            # Example: log the fallback action\n",
    "            logger.info(f\"Executing fallback: {fallback}\")\n",
    "            # Add actual fallback logic here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Configuration Loading\n",
    "Build functions to load and parse YAML configuration files that define data quality expectations and execution parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml_config(file_path):\n",
    "    \"\"\"\n",
    "    Load and parse a YAML configuration file.\n",
    "\n",
    "    :param file_path: Path to the YAML configuration file.\n",
    "    :return: Parsed configuration as a dictionary.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"YAML configuration file not found: {file_path}\")\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    \n",
    "    logger.info(f\"YAML configuration loaded successfully from {file_path}.\")\n",
    "    return config\n",
    "\n",
    "def validate_config(config):\n",
    "    \"\"\"\n",
    "    Validate the structure and required fields of the YAML configuration.\n",
    "\n",
    "    :param config: Parsed YAML configuration as a dictionary.\n",
    "    :return: None. Raises ValueError if validation fails.\n",
    "    \"\"\"\n",
    "    required_keys = ['expectations', 'execution_strategy']\n",
    "    for key in required_keys:\n",
    "        if key not in config:\n",
    "            raise ValueError(f\"Missing required configuration key: {key}\")\n",
    "    \n",
    "    logger.info(\"YAML configuration validated successfully.\")\n",
    "\n",
    "# Example usage\n",
    "try:\n",
    "    # Replace 'path/to/config.yaml' with the actual path to your YAML file\n",
    "    config_path = 'path/to/config.yaml'\n",
    "    config = load_yaml_config(config_path)\n",
    "    validate_config(config)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading or validating configuration: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Execution Strategies\n",
    "Implement different execution strategies (sequential, parallel, micro-batch) as separate classes with a common interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "class ExecutionStrategy:\n",
    "    \"\"\"\n",
    "    Abstract base class for execution strategies.\n",
    "    \"\"\"\n",
    "    def execute(self, dataframe: DataFrame, expectations: list):\n",
    "        \"\"\"\n",
    "        Execute the expectations on the given DataFrame.\n",
    "        :param dataframe: Spark DataFrame to apply expectations to.\n",
    "        :param expectations: List of expectations to apply.\n",
    "        :return: List of results for each expectation.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement the execute method.\")\n",
    "\n",
    "class SequentialExecutionStrategy(ExecutionStrategy):\n",
    "    \"\"\"\n",
    "    Execute expectations sequentially.\n",
    "    \"\"\"\n",
    "    def execute(self, dataframe: DataFrame, expectations: list):\n",
    "        results = []\n",
    "        for expectation in expectations:\n",
    "            result = self._apply_expectation(dataframe, expectation)\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "    def _apply_expectation(self, dataframe: DataFrame, expectation: dict):\n",
    "        # Example logic for applying an expectation\n",
    "        logger.info(f\"Applying expectation: {expectation['name']}\")\n",
    "        # Simulate expectation result\n",
    "        return {\"name\": expectation['name'], \"status\": \"passed\"}\n",
    "\n",
    "class ParallelExecutionStrategy(ExecutionStrategy):\n",
    "    \"\"\"\n",
    "    Execute expectations in parallel using ThreadPoolExecutor.\n",
    "    \"\"\"\n",
    "    def execute(self, dataframe: DataFrame, expectations: list):\n",
    "        results = []\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(self._apply_expectation, dataframe, expectation) for expectation in expectations]\n",
    "            for future in futures:\n",
    "                results.append(future.result())\n",
    "        return results\n",
    "\n",
    "    def _apply_expectation(self, dataframe: DataFrame, expectation: dict):\n",
    "        # Example logic for applying an expectation\n",
    "        logger.info(f\"Applying expectation in parallel: {expectation['name']}\")\n",
    "        # Simulate expectation result\n",
    "        return {\"name\": expectation['name'], \"status\": \"passed\"}\n",
    "\n",
    "class MicroBatchExecutionStrategy(ExecutionStrategy):\n",
    "    \"\"\"\n",
    "    Execute expectations in micro-batches.\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size: int):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def execute(self, dataframe: DataFrame, expectations: list):\n",
    "        results = []\n",
    "        for i in range(0, len(expectations), self.batch_size):\n",
    "            batch = expectations[i:i + self.batch_size]\n",
    "            logger.info(f\"Processing batch of size {len(batch)}\")\n",
    "            for expectation in batch:\n",
    "                result = self._apply_expectation(dataframe, expectation)\n",
    "                results.append(result)\n",
    "        return results\n",
    "\n",
    "    def _apply_expectation(self, dataframe: DataFrame, expectation: dict):\n",
    "        # Example logic for applying an expectation\n",
    "        logger.info(f\"Applying expectation in micro-batch: {expectation['name']}\")\n",
    "        # Simulate expectation result\n",
    "        return {\"name\": expectation['name'], \"status\": \"passed\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Data Quality Expectations\n",
    "Create a framework for defining various data quality checks (completeness, uniqueness, range validation, etc.) that can be applied to Spark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "class DataQualityExpectation:\n",
    "    \"\"\"\n",
    "    Base class for data quality expectations.\n",
    "    \"\"\"\n",
    "    def validate(self, dataframe: DataFrame):\n",
    "        \"\"\"\n",
    "        Validate the expectation on the given DataFrame.\n",
    "        :param dataframe: Spark DataFrame to validate.\n",
    "        :return: Dictionary with validation result.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement the validate method.\")\n",
    "\n",
    "class CompletenessExpectation(DataQualityExpectation):\n",
    "    \"\"\"\n",
    "    Check for completeness of a column (no null values).\n",
    "    \"\"\"\n",
    "    def __init__(self, column_name: str):\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def validate(self, dataframe: DataFrame):\n",
    "        null_count = dataframe.filter(col(self.column_name).isNull()).count()\n",
    "        total_count = dataframe.count()\n",
    "        status = \"passed\" if null_count == 0 else \"failed\"\n",
    "        return {\n",
    "            \"expectation\": f\"Completeness check for column {self.column_name}\",\n",
    "            \"status\": status,\n",
    "            \"details\": f\"{null_count} null values out of {total_count} rows\"\n",
    "        }\n",
    "\n",
    "class UniquenessExpectation(DataQualityExpectation):\n",
    "    \"\"\"\n",
    "    Check for uniqueness of a column.\n",
    "    \"\"\"\n",
    "    def __init__(self, column_name: str):\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def validate(self, dataframe: DataFrame):\n",
    "        total_count = dataframe.count()\n",
    "        unique_count = dataframe.select(self.column_name).distinct().count()\n",
    "        status = \"passed\" if total_count == unique_count else \"failed\"\n",
    "        return {\n",
    "            \"expectation\": f\"Uniqueness check for column {self.column_name}\",\n",
    "            \"status\": status,\n",
    "            \"details\": f\"{unique_count} unique values out of {total_count} rows\"\n",
    "        }\n",
    "\n",
    "class RangeExpectation(DataQualityExpectation):\n",
    "    \"\"\"\n",
    "    Check if values in a column fall within a specified range.\n",
    "    \"\"\"\n",
    "    def __init__(self, column_name: str, min_value: float, max_value: float):\n",
    "        self.column_name = column_name\n",
    "        self.min_value = min_value\n",
    "        self.max_value = max_value\n",
    "\n",
    "    def validate(self, dataframe: DataFrame):\n",
    "        out_of_range_count = dataframe.filter(\n",
    "            (col(self.column_name) < self.min_value) | (col(self.column_name) > self.max_value)\n",
    "        ).count()\n",
    "        total_count = dataframe.count()\n",
    "        status = \"passed\" if out_of_range_count == 0 else \"failed\"\n",
    "        return {\n",
    "            \"expectation\": f\"Range check for column {self.column_name} ({self.min_value} to {self.max_value})\",\n",
    "            \"status\": status,\n",
    "            \"details\": f\"{out_of_range_count} out-of-range values out of {total_count} rows\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Result Aggregation\n",
    "Develop methods to collect, aggregate, and analyze results from multiple quality checks, with support for blocking policies based on failure thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultAggregator:\n",
    "    \"\"\"\n",
    "    Class to collect, aggregate, and analyze results from multiple quality checks.\n",
    "    Supports blocking policies based on failure thresholds.\n",
    "    \"\"\"\n",
    "    def __init__(self, failure_threshold: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize the ResultAggregator with a failure threshold.\n",
    "        :param failure_threshold: Proportion of failed checks that triggers blocking.\n",
    "        \"\"\"\n",
    "        self.failure_threshold = failure_threshold\n",
    "\n",
    "    def aggregate_results(self, results: list):\n",
    "        \"\"\"\n",
    "        Aggregate and analyze the results of quality checks.\n",
    "        :param results: List of dictionaries containing the results of quality checks.\n",
    "        :return: Aggregated results and blocking decision.\n",
    "        \"\"\"\n",
    "        if not results:\n",
    "            raise ValueError(\"No results provided for aggregation.\")\n",
    "\n",
    "        # Count passed and failed checks\n",
    "        passed = sum(1 for result in results if result['status'] == 'passed')\n",
    "        failed = sum(1 for result in results if result['status'] == 'failed')\n",
    "        total = passed + failed\n",
    "\n",
    "        # Calculate failure rate\n",
    "        failure_rate = failed / total if total > 0 else 0\n",
    "\n",
    "        # Determine if blocking policy is triggered\n",
    "        block_execution = failure_rate > self.failure_threshold\n",
    "\n",
    "        logger.info(f\"Aggregation complete: {passed} passed, {failed} failed, failure rate: {failure_rate:.2%}\")\n",
    "        logger.info(f\"Blocking policy triggered: {block_execution}\")\n",
    "\n",
    "        return {\n",
    "            \"passed\": passed,\n",
    "            \"failed\": failed,\n",
    "            \"failure_rate\": failure_rate,\n",
    "            \"block_execution\": block_execution\n",
    "        }\n",
    "\n",
    "# Example usage of ResultAggregator\n",
    "try:\n",
    "    # Simulated results from quality checks\n",
    "    simulated_results = [\n",
    "        {\"name\": \"Check 1\", \"status\": \"passed\"},\n",
    "        {\"name\": \"Check 2\", \"status\": \"failed\"},\n",
    "        {\"name\": \"Check 3\", \"status\": \"passed\"},\n",
    "        {\"name\": \"Check 4\", \"status\": \"failed\"}\n",
    "    ]\n",
    "\n",
    "    # Initialize ResultAggregator with a failure threshold of 20%\n",
    "    aggregator = ResultAggregator(failure_threshold=0.2)\n",
    "\n",
    "    # Aggregate results and analyze\n",
    "    aggregated_results = aggregator.aggregate_results(simulated_results)\n",
    "    logger.info(f\"Aggregated Results: {aggregated_results}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during result aggregation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Fallback Mechanism\n",
    "Implement fallback procedures that trigger when quality checks fail, based on the configuration settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FallbackHandler:\n",
    "    \"\"\"\n",
    "    Class to handle fallback procedures when quality checks fail.\n",
    "    \"\"\"\n",
    "    def __init__(self, fallback_config: list):\n",
    "        \"\"\"\n",
    "        Initialize the FallbackHandler with the fallback configuration.\n",
    "        :param fallback_config: List of fallback procedures defined in the configuration.\n",
    "        \"\"\"\n",
    "        self.fallback_config = fallback_config\n",
    "\n",
    "    def execute_fallbacks(self):\n",
    "        \"\"\"\n",
    "        Execute the fallback procedures as defined in the configuration.\n",
    "        \"\"\"\n",
    "        if not self.fallback_config:\n",
    "            logger.info(\"No fallback procedures to execute.\")\n",
    "            return\n",
    "\n",
    "        for fallback in self.fallback_config:\n",
    "            action = fallback.get(\"action\")\n",
    "            params = fallback.get(\"params\", {})\n",
    "            logger.info(f\"Executing fallback action: {action} with params: {params}\")\n",
    "            self._execute_action(action, params)\n",
    "\n",
    "    def _execute_action(self, action: str, params: dict):\n",
    "        \"\"\"\n",
    "        Execute a specific fallback action.\n",
    "        :param action: The action to execute.\n",
    "        :param params: Parameters for the action.\n",
    "        \"\"\"\n",
    "        if action == \"log_message\":\n",
    "            message = params.get(\"message\", \"No message provided.\")\n",
    "            logger.warning(f\"Fallback action - Log Message: {message}\")\n",
    "        elif action == \"send_alert\":\n",
    "            alert_type = params.get(\"type\", \"email\")\n",
    "            recipient = params.get(\"recipient\", \"admin@example.com\")\n",
    "            logger.warning(f\"Fallback action - Send Alert: Type={alert_type}, Recipient={recipient}\")\n",
    "        elif action == \"retry_operation\":\n",
    "            retries = params.get(\"retries\", 3)\n",
    "            logger.warning(f\"Fallback action - Retry Operation: Retries={retries}\")\n",
    "            # Add retry logic here\n",
    "        else:\n",
    "            logger.error(f\"Unknown fallback action: {action}\")\n",
    "\n",
    "# Example usage of FallbackHandler\n",
    "try:\n",
    "    # Simulated fallback configuration\n",
    "    simulated_fallbacks = [\n",
    "        {\"action\": \"log_message\", \"params\": {\"message\": \"Data quality check failed.\"}},\n",
    "        {\"action\": \"send_alert\", \"params\": {\"type\": \"email\", \"recipient\": \"data_team@example.com\"}},\n",
    "        {\"action\": \"retry_operation\", \"params\": {\"retries\": 5}}\n",
    "    ]\n",
    "\n",
    "    # Initialize FallbackHandler with the simulated configuration\n",
    "    fallback_handler = FallbackHandler(simulated_fallbacks)\n",
    "\n",
    "    # Execute fallback procedures\n",
    "    fallback_handler.execute_fallbacks()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during fallback execution: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo with Example DataFrames\n",
    "Demonstrate the DataQualityManager with example Spark DataFrames, showing how to apply configurations and interpret results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo with Example DataFrames\n",
    "\n",
    "# Create example DataFrames\n",
    "example_data_1 = [\n",
    "    (1, \"Alice\", 25),\n",
    "    (2, \"Bob\", None),\n",
    "    (3, \"Charlie\", 30),\n",
    "    (4, \"David\", 35),\n",
    "    (5, \"Eve\", None)\n",
    "]\n",
    "example_data_2 = [\n",
    "    (1, \"Alice\", 1000),\n",
    "    (2, \"Bob\", 2000),\n",
    "    (3, \"Charlie\", 3000),\n",
    "    (4, \"David\", 4000),\n",
    "    (5, \"Eve\", 5000)\n",
    "]\n",
    "\n",
    "schema_1 = [\"id\", \"name\", \"age\"]\n",
    "schema_2 = [\"id\", \"name\", \"salary\"]\n",
    "\n",
    "df1 = spark.createDataFrame(example_data_1, schema_1)\n",
    "df2 = spark.createDataFrame(example_data_2, schema_2)\n",
    "\n",
    "# Display the example DataFrames\n",
    "logger.info(\"Example DataFrame 1:\")\n",
    "df1.show()\n",
    "\n",
    "logger.info(\"Example DataFrame 2:\")\n",
    "df2.show()\n",
    "\n",
    "# Define expectations for the demo\n",
    "demo_expectations = [\n",
    "    CompletenessExpectation(column_name=\"age\"),\n",
    "    UniquenessExpectation(column_name=\"id\"),\n",
    "    RangeExpectation(column_name=\"age\", min_value=20, max_value=40)\n",
    "]\n",
    "\n",
    "# Apply expectations to the first DataFrame\n",
    "logger.info(\"Applying expectations to DataFrame 1...\")\n",
    "results_df1 = [expectation.validate(df1) for expectation in demo_expectations]\n",
    "\n",
    "# Log results for DataFrame 1\n",
    "logger.info(\"Results for DataFrame 1:\")\n",
    "for result in results_df1:\n",
    "    logger.info(result)\n",
    "\n",
    "# Apply expectations to the second DataFrame\n",
    "logger.info(\"Applying expectations to DataFrame 2...\")\n",
    "results_df2 = [expectation.validate(df2) for expectation in demo_expectations]\n",
    "\n",
    "# Log results for DataFrame 2\n",
    "logger.info(\"Results for DataFrame 2:\")\n",
    "for result in results_df2:\n",
    "    logger.info(result)\n",
    "\n",
    "# Aggregate results using ResultAggregator\n",
    "aggregator = ResultAggregator(failure_threshold=0.2)\n",
    "\n",
    "logger.info(\"Aggregating results for DataFrame 1...\")\n",
    "aggregated_results_df1 = aggregator.aggregate_results(results_df1)\n",
    "logger.info(f\"Aggregated Results for DataFrame 1: {aggregated_results_df1}\")\n",
    "\n",
    "logger.info(\"Aggregating results for DataFrame 2...\")\n",
    "aggregated_results_df2 = aggregator.aggregate_results(results_df2)\n",
    "logger.info(f\"Aggregated Results for DataFrame 2: {aggregated_results_df2}\")\n",
    "\n",
    "# Handle fallbacks if necessary\n",
    "fallback_config = [\n",
    "    {\"action\": \"log_message\", \"params\": {\"message\": \"Fallback triggered for DataFrame 1\"}},\n",
    "    {\"action\": \"send_alert\", \"params\": {\"type\": \"email\", \"recipient\": \"data_team@example.com\"}}\n",
    "]\n",
    "\n",
    "fallback_handler = FallbackHandler(fallback_config)\n",
    "\n",
    "if aggregated_results_df1[\"block_execution\"]:\n",
    "    logger.warning(\"Blocking policy triggered for DataFrame 1. Executing fallbacks...\")\n",
    "    fallback_handler.execute_fallbacks()\n",
    "\n",
    "if aggregated_results_df2[\"block_execution\"]:\n",
    "    logger.warning(\"Blocking policy triggered for DataFrame 2. Executing fallbacks...\")\n",
    "    fallback_handler.execute_fallbacks()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
